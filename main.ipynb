{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/24 12:03:44 WARN Utils: Your hostname, fossa-dsa-001 resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp3s0)\n",
      "22/06/24 12:03:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fossa/deb/tests/spark_fun/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/24 12:03:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\"\"\"make an instance of a SparkSession called 'spark'.\"\"\"\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Project Setup:\"\"\"\n",
    "# In a main.ipynb file, make an instance of a SparkSession called 'spark'.\n",
    "# In the header of the data, give the index column a name. Otherwise, you'll get the error \"CSVHeaderChecker: CSV header does not conform to the schema\".\n",
    "# Read the Spotify artists CSV file into a Spark DataFrame with the following snippet:\n",
    "# spark_df = (spark.read.format(\"csv\").options(header=\"true\").load(\"../project-1/data/spotify_artists.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read the Spotify artists CSV file into a Spark DataFrame\n",
    "spark_df = (spark.read.format(\"csv\").options(header=\"true\").load(\"data/spotify_artists.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[_c0: string, artist_popularity: string, followers: string, genres: string, id: string, name: string, track_id: string, track_name_prev: string, type: string]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Profile the Data:\"\"\"\n",
    "# Show a description (summary) of the Spark DataFrame.\n",
    "spark_df.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3717256039.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_68580/3717256039.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    StructField('index', IntegerType, true),\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "\n",
    "custom_schema = StructType() \\\n",
    "    StructField('index', IntegerType, true),\n",
    "    StructField('artist_popularity', IntegerType, true),\n",
    "    StructField('followers', IntegerType, true),\n",
    "    StructField('genres', StringType, true),\n",
    "    StructField('id', StringType, true),\n",
    "    StructField('name', StringType, true),\n",
    "    StructField('track_id', StringType, true),\n",
    "    StructField('track_name_prev', StringType, true),\n",
    "    StructField('type', StringType, true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- artist_popularity: string (nullable = true)\n",
      " |-- followers: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name_prev: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame.\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68580/588530822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select and show just the first 10 values in the 'name' and 'genres' columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "# Select and show just the first 10 values in the 'name' and 'genres' columns.\n",
    "spark_df.select(spark_df.name, spark_df.genres).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8793d36fc6f82970c1c1d8f760fcf31a9b83e2b77c85ced74640d867ec7076d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
